import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import numpy as np
import math
import random
'''Author: Jonathan Lutch
   Date: 11/29/2022
   About: This is a heavily edited version of a program I wrote for my
   AI class. It's main purpose is to generate an animation that shows a
   neural network "learning" how to classify two sets of points. That being
   said, it can be tweaked to display other information, such as a graph of
   the objective error of the neural net as a function of gradient descent
   iterations.
'''
def graphIrisData(irisData, species):
	'''Graph all irises that are of the species versicolor and virginica'''
	versicolorLabel = "versicolor"
	virginicaLabel = "virginica"
	for node in range(len(irisData[0])):
		if species[node] == "versicolor":
			plt.scatter(irisData[0][node], irisData[1][node], marker = "*", color = "g", label = versicolorLabel)
			#otherwise we will have 100 labels in the legend!
			versicolorLabel = "_versicolore"
		elif species[node] == "virginica":
			plt.scatter(irisData[0][node], irisData[1][node], marker = "+", color = "b", label = virginicaLabel)
			virginicaLabel = "_virginica"
	plt.legend()
	plt.xlabel("Petal Length (cm)")
	plt.ylabel("Petal Width (cm)")
	plt.title("Iris Species by Petal Length and Width")



def neuralNetworkNode(weights, inputs):
	'''Return the value of the sigmoid function where the
	input is the dot product of 'weights' and 'inputs'.
	Assumes that bias is the last weight and that the last
	input is 1.'''
	mySum = 0
	for index in range(len(weights)):
		mySum += weights[index] * inputs[index]
	return sigmoid(mySum)

def sigmoid(mySum):
	'''Return the value of the sigmoid function
	where the input is 'mySum'.'''
	return 1 / (1 + math.exp(-mySum))

# # def graph3D(weights, irisLength):
# # 	'''Graph the sigmoid function over the input space
# # 	using the weights assigned for each demension in the
# # 	input space.'''
# # 	x = np.linspace(min(irisLength), max(irisLength), 100)
# # 	y = np.linspace(0, 3, 100)
# # 	X, Y = np.meshgrid(x, y)
	
# 	#getting a z value for every point (X,Y)
# 	Z = []
# 	for row in range(len(X)):
# 		tmp = []
# 		for col in range(len(X[0])):
# 			tmp.append(neuralNetworkNode(weights, [X[row][col], Y[row][col], 1]))
# 		Z.append(tmp)
# 	Z = np.array(Z)

# 	#plotting graph
# 	fig = plt.figure()
# 	ax = plt.axes(projection='3d')
# 	ax.plot_surface(X, Y, Z, rstride = 1, cstride = 1, cmap = "viridis", edgecolor = "none")
# 	ax.set_xlabel("Petal Length (cm)")
# 	ax.set_ylabel("Petal Width (cm)")
# 	ax.set_zlabel("Sigmoid Value")
# 	ax.set_title("Sigmoid Value as a Function of Petal Length and Petal Width")

def getData(irisData, node):
	'''Return a list of the data related to a certain node.'''
	myData = []
	for index in range(0, len(irisData)):
		myData.append(irisData[index][node])
	myData.append(1)
	return myData

def calculateMeanSquaredError(irisData, species, weights):
	'''Return the mean squared error generated by the boundary
	defined by the current weights.'''
	mySum = 0
	#for each node
	for node in range(0, len(irisData[0])):
		myClass = 0
		if species[node] == "virginica":
			myClass = 1
		myData = getData(irisData, node)
		mySum += (neuralNetworkNode(weights, myData) - myClass) ** 2
	return mySum / len(irisData[0])

def calculateGradient(irisData, species, weights, dimension):
	'''Calculate and return the gradient for the weight "weights[dimension]".'''
	mySum = 0
	for node in range(0, len(irisData[0])):
		myClass = 0
		if species[node] == "virginica":
			myClass = 1
		x = None
		if dimension == len(weights) - 1:
			x = 1
		else:
			x = irisData[dimension][node]
		myData = getData(irisData, node)
		#calculating parts 1,2,3 of the gradient summation
		part1 = (neuralNetworkNode(weights, myData) - myClass)
		part2 = neuralNetworkNode(weights, myData) - ((neuralNetworkNode(weights, myData)) ** 2)
		part3 = x
		mySum += part1*part2*part3
	return mySum / len(irisData[0]) * 2

def graphBoundaryLine(weights, irisLength):
	'''Graph a line where values to the left of the line are less
	than 0, values on the line are 0, and values to the right of the
	line are greater than 0.'''
	x = np.linspace(min(irisLength), max(irisLength), 1000)
	y = -(weights[0] * x + weights[2]) / weights[1]  
	plt.ylim((0, 3))
	line.set_data((x,y))
	return line

def optimizeBoundaryWrapper(irisData, species, startingWeights, stepSize, tolerance, plotAt, line):
	'''Optimize boundary line weights using gradient descent.
		Program can be tweaked to show the objective error of the
		line, hence some of the seemingly unused variables.'''
	weightHistory = [startingWeights]
	objectiveValueHistory = []
	objectiveValue = 0
	oldObjectiveValue = None
	weights = startingWeights
	newWeights = [0] * len(weights)
	myTol = 1
	iterations = 0
	'''Optimize our weights until we are changing by an amount less than "
	tolerance" or we hit a local minimum'''
	while tolerance < myTol and objectiveValue != oldObjectiveValue:
		
		#updating objective function data and generating new objectiveValue
		oldObjectiveValue = objectiveValue
		objectiveValue = calculateMeanSquaredError(irisData, species, weights)
		objectiveValueHistory.append(objectiveValue)
		myTol = ((oldObjectiveValue - objectiveValue) ** 2) ** 0.5
		#print(objectiveValue)
		#print("Weights: " + str(weights))
		#for each weight...
		temp = []
		for demension in range(0, len(weights)):
			#for each demension, we want to change the weights slighty
			gradient = calculateGradient(irisData, species, weights, demension)
			newWeights[demension] = weights[demension] - stepSize * gradient
			temp.append(newWeights[demension])
		weights = newWeights
		weightHistory.append(temp)
		
		iterations += 1
	return weightHistory, objectiveValueHistory

def showGradientDescent(irisData, weights, line):
	'''Show how program does gradient descent using objective
	value graphs and descion boundary graphs.'''
	plotAt = []
	#The following are (sort of) arbitrarily set, feel free to change
	stepSize = .01
	tolerance = 0.000001
	weightHistory, objectiveValueHistory = optimizeBoundaryWrapper(irisData, species,
											   weights, stepSize, tolerance, plotAt, line)
	return weightHistory
	
if __name__ == "__main__":
	df = pd.read_csv("irisdata.csv")
	df = df[df.species != "setosa"]
	df.reset_index(drop = True, inplace = True)
	#Now irisData does not include setosas!
	species = df["species"]
	irisData = [df["petal_length"], df["petal_width"]]

	#lets spend some time setting up our plot
	fig = plt.figure()
	lines = plt.plot([])
	line = lines[0]
	graphIrisData(irisData, species)
	
	'''These are just random starting weights. There are actually 4
		paramaters we could use, but if want a nice 2D graph we will
		need to use only 2. Uncomment some of the other ones if you want
		to see how they converge!'''
	#weights = [1.35, 2, -7]
	#weights = [-1.35, 6, -7]
	weights = [1.55, 2.7, -8]
	
	
	print("Starting animation")
	weightHistory = showGradientDescent(irisData, weights, line)
	
	def animation(frame):
		graphBoundaryLine(weightHistory[frame], irisData[0])
	
	anim = FuncAnimation(fig, animation, frames = len(weightHistory) - 1, interval = 60)
	plt.show()
	plt.close()
	print("Done")